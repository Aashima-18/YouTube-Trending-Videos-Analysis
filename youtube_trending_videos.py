# -*- coding: utf-8 -*-
"""YouTube Trending Videos.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12NikhYqcp5wpxcuMg21QOEke0TLaUnkt
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plp

from google.colab import files
files.upload()

df = pd.read_csv('US_youtube_trending_data.csv')

df.head()

df.info()

#-- missing data-

df.isnull().sum()

from datetime import datetime

# remove duplicates--
df = df.drop_duplicates()

df.to_csv('cleaned_youtube_trending.csv', index=False)

df.describe()

# column names-

if not df.empty:
    print("Column names in the dataset:")
    print(df.columns.tolist())

# datetime format--

if not df.empty:
    date_columns = [col for col in df.columns if 'date' in col.lower()]
    publish_columns = [col for col in df.columns if 'publish' in col.lower()]

    print(f"Date columns found: {date_columns}")
    print(f"Publish columns found: {publish_columns}")

if 'trending_date' in df.columns:
        # Check the format of trending_date
        print("Sample trending_date values:")
        print(df['trending_date'].head())

try:
    df['trending_date'] = pd.to_datetime(df_us['trending_date'], format='%y.%d.%m')
except:
    try:
        df['trending_date'] = pd.to_datetime(df_us['trending_date'])
    except:
        print("Could not convert trending_date to datetime.")

if 'publishedAt' in df.columns:
        try:
            df['publishedAt'] = pd.to_datetime(df['publishedAt'])
        except:
            print("Could not convert publishedAt to datetime.")

if not df.empty:
  if 'tags' in df.columns:
        # Parse tags from string format to list
        df['tags_list'] = df['tags'].apply(lambda x: [] if x == '[none]' else str(x).strip('"').split('|'))

df['tags_count'] = df['tags_list'].apply(len)

if 'duration' in df.columns:
    def extract_duration_seconds(duration_str):
        if not isinstance(duration_str, str):
            return 0

        minutes = re.search(r'(\d+)M', duration_str)
        seconds = re.search(r'(\d+)S', duration_str)
        hours = re.search(r'(\d+)H', duration_str)

        total_seconds = 0
        if hours:
            total_seconds += int(hours.group(1)) * 3600
        if minutes:
            total_seconds += int(minutes.group(1)) * 60
        if seconds:
            total_seconds += int(seconds.group(1))

        return total_seconds

    df['duration_seconds'] = df['duration'].apply(extract_duration_seconds)

if not df.empty:
    engagement_columns = ['view_count', 'likes', 'dislikes', 'comment_count']

existing_columns = [col for col in engagement_columns if col in df.columns]
print(f"Engagement columns found: {existing_columns}")

for col in existing_columns:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')

if 'view_count' in df.columns and df['view_count'].sum() > 0:
        if 'likes' in df.columns:
            df['likes_ratio'] = df['likes'] / df['view_count']

        if 'dislikes' in df.columns:
            df['dislikes_ratio'] = df['dislikes'] / df['view_count']

        if 'comment_count' in df.columns:
            df['comments_ratio'] = df['comment_count'] / df['view_count']

if all(col in df.columns for col in ['likes_ratio', 'dislikes_ratio', 'comments_ratio']):
    df['engagement_score'] = df['likes_ratio'] + df['comments_ratio'] - df['dislikes_ratio']
elif 'likes_ratio' in df.columns and 'comments_ratio' in df.columns:
    df['engagement_score'] = df['likes_ratio'] + df['comments_ratio']

df.groupby('categoryId')['view_count'].sum().sort_values(ascending=False)

df.replace([np.inf, -np.inf], np.nan, inplace=True)
df.fillna(0, inplace=True)

category_dict = {
    1: "Film & Animation",
    2: "Autos & Vehicles",
    10: "Music",
    15: "Pets & Animals",
    17: "Sports",
    18: "Short Movies",
    19: "Travel & Events",
    20: "Gaming",
    21: "Videoblogging",
    22: "People & Blogs",
    23: "Comedy",
    24: "Entertainment",
    25: "News & Politics",
    26: "Howto & Style",
    27: "Education",
    28: "Science & Technology",
    29: "Nonprofits & Activism",
    30: "Movies",
    31: "Anime/Animation",
    32: "Action/Adventure",
    33: "Classics",
    34: "Comedy",
    35: "Documentary",
    36: "Drama",
    37: "Family",
    38: "Foreign",
    39: "Horror",
    40: "Sci-Fi/Fantasy",
    41: "Thriller",
    42: "Shorts",
    43: "Shows",
    44: "Trailers"
}

df['category_name'] = df['categoryId'].map(category_dict)

df[['categoryId', 'category_name']].head()

# Top categories by video count--
category_counts = df['category_name'].value_counts().head(10)

category_counts.plot(kind='barh', title='Top 10 Categories by Video Count')
plt.show()

# avg likes, dislikes and comments per category--


avg_likes = df.groupby('category_name')['likes'].mean().sort_values(ascending=False)

avg_likes.plot(kind='bar', title='Average Likes by Category')
plt.show()

avg_comments = df.groupby('category_name')['comment_count'].mean().sort_values(ascending=False)

avg_comments.plot(kind='bar', title='Average Comments by Category')
plt.show()

# Correlation between likes, views, and comments
df[['view_count', 'likes', 'dislikes', 'comment_count']].corr()

# Top 5 videos by views--
top_videos = df.sort_values('view_count', ascending=False).head(5)
top_videos[['title', 'channelId', 'view_count', 'likes', 'dislikes', 'comment_count']]

df_grouped = df.groupby('trending_date')['view_count'].sum()

df_grouped.plot(title='Total Views Over Time')
plt.show()

# Top 10 channels by number of trending videos
df['title'].value_counts().head(10).plot(kind='barh', title='Top 10 Channels by Trending Videos')
plt.show()

from wordcloud import WordCloud

text = ' '.join(df['title'].dropna())
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

plt.figure(figsize=(10,5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Video Titles')
plt.show()

